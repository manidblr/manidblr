Create an export dumpfile
Create an S3 bucket

Amazon Resource Name (ARN)
arn:aws:s3:::dbamani29-oradump
s3://dbamani29-oradump


Create AWS Policy: Grant Listbucket, readbucket, writebuket to the policy
arn:aws:iam::473371257295:policy/rds-s3-integration-policy
Reference: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/oracle-s3-integration.html
Add ARN and mention "*" in object list to include all the objects.

Create AWS Role:  Assign the policy to the role
arn:aws:iam::473371257295:role/rds-s3-integration-role


Aassociate your IAM role with your DB instance:
Goto RDS: Under "Connectivity & Security Rules", 
"Manage IAM Roles",
Enter "rds-s3-integration-role" in text box, and select the Feature "S3_INTEGRATION"


Configure an option group for Amazon S3 integration:
Select Existing Options Group:
default:oracle-se2-19.     Unable to edit existing Options Group. Hence creating a new one.
Create new group oracle-se2-19-S3 and select S3_INTEGRATION option.


Modify the RDS instance rds-ora1-19c and apply the options group oracle-se2-19-S3 


Downloading files from an Amazon S3 bucket to an Oracle DB instance
SELECT rdsadmin.rdsadmin_s3_tasks.download_from_s3(
      p_bucket_name    =>  'dbamani29-oradump',       
      p_directory_name =>  'DATA_PUMP_DIR') 
   AS TASK_ID FROM DUAL;  
Output Task ID: 1628351568375-28

Below is to download from a specific folder.
SELECT rdsadmin.rdsadmin_s3_tasks.download_from_s3(
      p_bucket_name    =>  'mys3bucket', 
      p_s3_prefix      =>  'myfolder/', 
      p_directory_name =>  'DATA_PUMP_DIR') 
   AS TASK_ID FROM DUAL; 
   
 
 View the result using below query.
 SELECT text FROM table(rdsadmin.rds_file_util.read_text_file('BDUMP','dbtask-1628351568375-28.log')); 

To Do: Verify OEM Agent Option
Verify Upgrade process
Restore from S3.


# Creating tablespace in AWS RDS:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.Database.html
CREATE TABLESPACE USERS_TS DATAFILE SIZE 1G AUTOEXTEND ON MAXSIZE 10G;
This will create a regular tablespace with datafile as below. We dont have to give data file name. Oracle 
/rdsdbdata/db/ORCL_A/datafile/o1_mf_users_ts_jjxkfv5z_.dbf
alert_ORCL.log
db_create_file_dest      = "/rdsdbdata/db"


#Exporting and importing using S3.
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Oracle.Procedural.Importing.html#Oracle.Procedural.Importing.DataPump.S3

Export :
DECLARE
  v_hdnl NUMBER;
BEGIN
  v_hdnl := DBMS_DATAPUMP.OPEN(operation => 'EXPORT', job_mode => 'SCHEMA', job_name=>null);
  DBMS_DATAPUMP.ADD_FILE( 
    handle    => v_hdnl, 
    filename  => 'sample.dmp', 
    directory => 'DATA_PUMP_DIR', 
    filetype  => dbms_datapump.ku$_file_type_dump_file);
  DBMS_DATAPUMP.ADD_FILE( 
    handle    => v_hdnl, 
    filename  => 'sample_exp.log', 
    directory => 'DATA_PUMP_DIR', 
    filetype  => dbms_datapump.ku$_file_type_log_file);
  DBMS_DATAPUMP.METADATA_FILTER(v_hdnl,'SCHEMA_EXPR','IN (''SCHEMA_1'')');
  DBMS_DATAPUMP.METADATA_FILTER(v_hdnl,'EXCLUDE_NAME_EXPR',
    q'[IN (SELECT NAME FROM sys.OBJ$ WHERE TYPE# IN (66,67,74,79,59,62,46) AND OWNER# IN 
      (SELECT USER# FROM SYS.USER$ WHERE NAME IN 
        ('RDSADMIN','SYS','SYSTEM','RDS_DATAGUARD','RDSSEC')))]','PROCOBJ');
  DBMS_DATAPUMP.START_JOB(v_hdnl);
END;
/


Import:
##DBMS_DATAPUMP.METADATA_FILTER(v_hdnl,'SCHEMA_EXPR','IN (''SCHEMA_1'')');


DECLARE
  v_hdnl NUMBER;
BEGIN
  v_hdnl := DBMS_DATAPUMP.OPEN( 
    operation => 'IMPORT', 
    job_mode  => 'SCHEMA', 
    job_name  => null);
  DBMS_DATAPUMP.ADD_FILE( 
    handle    => v_hdnl, 
    filename  => 'Oracle11g_Sample_Schemas.zip', 
    directory => 'DATA_PUMP_DIR', 
    filetype  => dbms_datapump.ku$_file_type_dump_file);
  DBMS_DATAPUMP.ADD_FILE( 
    handle    => v_hdnl, 
    filename  => 'import_Oracle11g_Sample_Schemas.log', 
    directory => 'DATA_PUMP_DIR', 
    filetype  => dbms_datapump.ku$_file_type_log_file);
   DBMS_DATAPUMP.METADATA_FILTER(v_hdnl,'SCHEMA_EXPR','IN (''HR'')');
  DBMS_DATAPUMP.START_JOB(v_hdnl);
END;
/ 

Example 2:
Export:
declare
 handle number;
begin
 handle := dbms_datapump.open('EXPORT','SCHEMA');
 dbms_datapump.add_file(handle,'SCOTT3.DMP','DUMPDIR');
 dbms_datapump.metadata_filter(handle,'SCHEMA_EXPR','= ''SCOTT''');
 dbms_datapump.set_parallel(handle,4);
 dbms_datapump.start_job(handle);
 dbms_datapump.detach(handle);
end;
/ 

